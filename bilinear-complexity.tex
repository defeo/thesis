We have presented in Section~\ref{sec:complexity-models} an abstract model made
to understand the asymptotic behaviour of our algorithms. In this chapter, we
present an alternative notion of complexity called \emph{bilinear complexity},
where the focus is on the number of multiplications needed to compute some map.
\minitoc

\section{Bilinear complexity}

In the \emph{algebraic complexity model}~\cite{BCS13}, we assume that our
machine is able to perform any operation in some base field $\K$ in constant,
unit time. This is an idealized model made in order to simplify the
computation of the complexity of algebraic algorithms. Nevertheless,
multiplication of two variable quantities in $\K$ is arguably more expensive
than addition, or than multiplication of a variable by a fixed constant. In the
context of the computation of bilinear maps, extensive work has been done to
reduce the number of two-variable multiplications involved. Notable examples are
Karatsuba's algorithm~\cite{Karatsuba63} and
Strassen's algorithm~\cite{Strassen69}. Karatsuba's algorithm is
based on the fact that the bilinear map associated to the product of two
polynomials of degree $1$
\[
  A = a_1 X + a_0\text{ and }B = b_1 X + b_0
\]
can be computed with three products
\[
  c_0 = a_0b_0,
\]
\[
  c_1 = (a_0+a_1)(b_0+b_1),
\]
and
\[
  c_\infty = a_1b_1,
\]
instead
of the four classic ones $a_0b_0$, $a_0b_1$, $a_1b_0$ and $a_1b_1$ as follows:
\[
  AB = c_\infty X^2 + (c_1-c_\infty-c_0) X + c_0.
\]
It will become clear in Section~\ref{sec:evalinter} why we use the index
$\infty$ instead of $2$ for $c_\infty = a_1b_1$. Strassen's algorithm
exploits a similar idea in the case of $2\times2$ matrices: only $7$ products
are used instead of $8$ in order to compute a matrix product. Both these
algorithms have very practical consequences. Karatsuba's algorithm is used in
computer algebra softwares, when the standard multiplication is no longer
optimal, and when the Discrete Fourier Transform (DFT) is not yet the fastest.
Strassen's algorithm, used reccursively, is the fastest strategy available for
large matrix multiplication. Both these questions are treated in~\cite{GG13}.
Thus the idea of minimizing the number of multiplications, even if it means
having to compute more additions and substractions, seems a good idea.

The \emph{bilinear complexity}
$\mu(\Phi)$ of a bilinear map $\Phi$ over $\K$ represents the minimum number of two-variable
multiplications in a formula that computes $\Phi$, discarding the cost of other
operations such as addition or multiplication by a constant. In other words, in
this model of computation, we only count $2$-variable multiplications, and other
operations are assumed to have no cost. It is motivated by the fact that
$2$-variable multiplication is often more expensive to compute than other
operations and by the practicality of algorithms minimizing the multiplications,
such as Karatsuba's and Strassen's.
In particular when $\A$ is a finite dimensional algebra over $\K$,
we define the bilinear complexity of $\A$ as $\mu(\A/\K)=\mu(m_{\A})$
where $m_{\A}:\A\times\A\to\A$ is the multiplication map in $\A$ seen
as a $\K$-bilinear map.

Let $\K^{2\times2}$ be the algebra
of $2\times2$ matrices over $\K$. We know thanks to Strassen's algorithm that
\[
  \mu(\K^{2\times 2}/\K) \leq 7.
\]
In fact, this is optimal, so we have exactly $\mu(\K^{2\times2}/\K)=7$. In
general, it seems to be hard to find the bilinear complexity of a given algebra,
for example the bilinear complexity of $\K^{3\times3}$ is not known.
In the litterature, work has been done both to algorithmically find the bilinear complexity of
small algebras~\cite{BDEZ12, Covanov19} and to understand how the bilinear
complexity asymptotically grows~\cite{CC88, BCPRRR19}. Chudnovsky and Chudnovsky
proved in 1988 that the bilinear complexity of an extension field
$\mathbb{F}_{q^k}/\mathbb{F}_{q}$ is linear in the degree $k$ of the
extension, using an evaluation-interpolation method on curves. We present this
method in Section~\ref{sec:evalinter}.

\paragraph{Bilinear formulas.}

We can precisely define bilinear complexity with \emph{bilinear formulas}. We
also sometimes use the terms \emph{bilinear decomposition}, or \emph{bilinear
algorithm}, but it is really the same notion.
\begin{defi}[Bilinear formula]
  Let $V_1, V_2$ and $W$ be three finite dimensional vector spaces over $\K$ and 
  \[
    \Phi:V_1\times V_2\to W
  \]
  a bilinear map. A \emph{bilinear fomula}, or \emph{bilinear decomposition}, or
  \emph{bilinear algorithm} of length $n$ for $\Phi$ is a
  collection of $2n$ linear forms $\varphi_1, \dots, \varphi_n$ and $\psi_1,
  \dots, \psi_n$, and $n$ vectors $w_1, \dots, w_n$ in $W$ such that for all
  $x\in V_1$ and $y\in V_2$, we have
  \[
    \Phi(x, y) = \sum_{j=1}^n \varphi_j(x)\psi_j(y)w_j.
  \]
\end{defi}
\begin{defi}[Bilinear complexity]
  Let $V_1, V_2$ and $W$ be three finite dimensional vector spaces over $\K$ and 
  \[
    \Phi:V_1\times V_2\to W
  \]
  a bilinear map. The \emph{bilinear complexity} 
  \[
    \mu(\Phi)
  \]
  of $\Phi$ is the minimal length $n$ of a bilinear formula for $\Phi$.
\end{defi}
% TODO: explain the link between this definition and the previous definition
% where we said that it was the number of 2-variable multiplication
Equivalently, we can define the bilinear complexity as the rank of the tensor in 
\[
  V_1^\vee \times V_2^\vee \times W
\]
corresponding to $\Phi$, where $V_1^\vee$ (resp. $V_2^\vee$) is the dual space
of $V_1$ (resp. $V_2$). When the spaces $V_1$ and $V_2$ are equal
\[
  V_1 = V_2 = V
\]
the bilinear maps 
\[
  \Phi:V\times V\to W
\]
can be symmetric, \ie they can verify that, for all $x, y\in V$
\[
  \Phi(x, y) = \Phi(y, x).
\]
In that case, it is natural to investigate the existence and the length of
\emph{symmetric bilinear formulas}, \ie bilinear formulas where the linear forms
$\varphi_j$ and $\psi_j$ are equal, for all $j$. On an algorithmtic point of
view, it should also be easier to find all such formulas because the
search space is smaller. It is also easier to represent such formulas because we
only need to store $n$ linear forms instead of $2n$.
\begin{defi}[Symmetric bilinear formula]
  \label{def:sym-bil-for}
  Let $V$ and $W$ be two finite dimensional vector spaces over $\K$ and 
  \[
    \Phi:V\times V\to W
  \]
  a symmetric bilinear map. A \emph{symmetric bilinear fomula}, or
  \emph{symmetric bilinear decomposition}, or
  \emph{symmetric bilinear algorithm} of length $n$ for $\Phi$ is a
  collection of $n$ linear forms $\varphi_1, \dots, \varphi_n$
  and $n$ vectors $w_1, \dots, w_n$ in $W$ such that for all
  $x, y\in V$, we have
  \[
    \Phi(x, y) = \sum_{j=1}^n \varphi_j(x)\varphi_j(y)w_j.
  \]
\end{defi}
\begin{defi}[Symmetric bilinear complexity]
  \label{def:sym-bil-com}
  Let $V$ and $W$ be two finite dimensional vector spaces over $\K$ and 
  \[
    \Phi:V\times V\to W
  \]
  a bilinear map. The \emph{symmetric bilinear complexity} 
  \[
    \musym(\Phi)
  \]
  of $\Phi$ is the minimal length $n$ of a symmetric bilinear formula for
  $\Phi$.
\end{defi}
Note that it is not clear from Definition~\ref{def:sym-bil-for} that a
\emph{symmetric} bilinear formula always
exists for symmetric bilinear maps, but it is indeed
true~\cite[Lemma $1.6$]{Randriam12}, thus
Definiton~\ref{def:sym-bil-com} makes sense. We are particularly
interested in algebras $\A$ of the form
\[
  \A = \mathbb{F}_{q^k}[T]/(T^l)
\]
and for that reason we introduce a special notation for the bilinear complexity
of those algebra
\[
  \mu_q(k, l) = \mu(\A/\K).
\]
Among these algebras, the case $l=1$, where the algebra $\A$ is a finite field
extension of $\mathbb{F}_q$ of degree $k$ also plays a special role, so we
define 
\[
  \mu_q(k) = \mu_q(k, 1).
\]
Because these algebras are all commutative, the product 
\[
\begin{array}{lccl}
  m_\A:&\A\times \A&\to&\A\\
  &(x, y)&\mapsto&xy
\end{array}
\]
is a symmetric bilinear map, and we define the symmetric bilinear complexity of
the algebra $\A$ as the symmetric bilinear complexity of $m_\A$
\[
  \musym(\A) = \musym(m_\A).
\]
We also define the quantities
\[
  \musym_q(k, l)
\]
and 
\[
  \musym_q(k)
\]
the same way it was done for the usual bilinear complexity. Since a symmetric
bilinear formula is in particular a bilinear formula, we have for all $k\geq1$
and $l\geq1$
\[
  \mu_q(k, l)\leq\musym_q(k, l).
\]
In the other direction, we know (\cite[Theorem $1$]{SL84} or \cite[Lemma
$1.6$]{Randriam12}) that when the characteristic of $\A$ is not $2$,
or equivalently when $k$ is not a power of $2$ , that we have
\[
  \musym_q(k, l)\leq2\mu_q(k, l).
\]
Finally, we have no example of algebra $\A = \mathbb{F}_{q^k}[T]/(T^l)$ where the
quantities $\mu_q(k, l)$ and $\musym_q(k, l)$ are different when $q\geq3$.
% TODO: elaborate on this, is it true for q = 2?
% see for example
% [ 0 1 ]
% [ 1 0 ]

\section{Chudnovsky-Chudnovsky algorithm}
Chudnovsky and Chudnovsky algorithm is based on evaluation-interpolation on
curves, we thus begin by presenting this principle.
\subsection{Evaluation - Interpolation}
\label{sec:evalinter}

Let $P\in\K[x]$ be a polynomial with coefficients in a finite field $\K$. The
evaluation-interpolation strategy is based on two facts:
\begin{itemize}
  \item a polynomial of degree $n$ can be described by its values at $n+1$
    points and reconstructed via \emph{interpolation};
  \item the \emph{evaluation} map at some point $a\in\K$ is a homomorphism of ring from
    $\K[x]$ to $\K$.
\end{itemize}
\paragraph{Interpolation.} The fact that a polynomial $P\in\K[x]$ of degree $n$
is uniquely determined by its values at $n+1$ (different) points in $\K$ follows
from the fact that a polynomial of degree $n$ with coefficients in $\K$ has up
to $n$ roots. This gives us the \emph{uniqueness} of the polynomial. As for the
\emph{existence}, it follows from the Lagrange interpolation. Let $x_1, \dots,
x_{n+1}\in\K$ be $n+1$ points in $\K$ and $y_1, \dots, y_{n+1}$ the
corresponding evaluation values, such that
\[
  \forall j\in\left\{ 1, \dots, n+1 \right\},\,y_j = P(x_j).
\]
Let 
\[
  L_j = \prod_{i\neq j}\frac{x-x_i}{x_j-x_i},
\]
we then have $L_j(x_i) = \delta_{i, j}$ with
\[
  \delta_{i, j} = 
  \left\{\begin{array}{ll}
      1&\mbox{if } i=j\\
      0&\mbox{if } i\neq j
    \end{array}
    \right.
\]
the Kronecker symbol. Now, the polynomial
\[
  P = \sum_{j=1}^{n+1} y_j L_j
\]
meets all the evaluation conditions and is the sum of polynomials of degree $n$
so $P$ is of degree at most $n$.

\paragraph{Evaluation.} Let $P, Q\in\K[x]$ be two polynomials with coefficients
in $\K$ and $a\in\K$, then we have
\[
  (P+_{\K[x]}Q)(a) = P(a) +_{\K} Q(a)
\]
and 
\[
  (P\times_{\K[x]} Q)(a) = P(a) \times_{\K} Q(a),
\]
where $+_{\K[x]}, \times_{\K[x]}$ (resp. $+_{\K}, \times_{\K})$ are the addition
and multiplication operations in $\K[x]$ (resp. $\K$).
In other words, the map
\[
\begin{array}{lccl}
  \textrm{ev}_a:&\K[x]&\to&\K\\
  &P&\mapsto&P(a)
\end{array}
\]
is an homomorphism of rings from $\K[x]$ to $\K$.

We are used to represent polynomials by their coefficients, but these two facts
suggest that we can also represent polynomials by their values at some points.
With this representation, adding two polynomials is done by adding the
values, which is done with linear complexity, the same as with the coefficient
representation. But the multiplication of polynomials is also obtained via the
multiplication of the values, which is linear again and better than the
quadratic complexity obtained with the usual multiplication formula for the
coefficients. An important problem is then to be able to change between
representations at a small cost, this is done using well-chosen points of
evaluation and this strategy is known under the name of Fast Fourier
Transform (FFT)~\cite{GG13}. Let $P, Q\in\K[x]$ be two polynomials with
coefficients in $\K$ represented by their coefficients, such that
$\deg(PQ)=n-1$. In order to multiply $P$ and $Q$ we need at least $n$ points in
$\K$ and the evaluation-interpolation strategy consists in $3$ steps:
\begin{enumerate}
  \item multipoint evaluation of $P$ and $Q$ at $n$ points $a_1, \dots, a_n$;
  \item coordinate-wise multiplication;
  \item interpolation to reconstruct the product $PQ$.
\end{enumerate}
When there are not enough points in $\K$ to use this method, instead of
evaluating on points of $\K$, we can evaluate the polynomials on points of
curves on $\K$ with enough points. As a first example, we can interpret
Karatsuba's algorithm as an evaluation-interpolation scheme on
the projective line $\mathbb{P}^1(\K)$. Let 
\[
  P = a_1 x + a_0
\]
and 
\[
  Q = b_1 x + b_0,
\]
then
\[
  c_0 = \textrm{ev}_0(P)\textrm{ev}_0(Q) = a_0b_0
\]
is obtained via evaluation at $0$,
\[
  c_1 = \textrm{ev}_1(P)\textrm{ev}_1(Q) = (a_0+a_1)(b_0+b_1)
\]
is obtained via evaluation at $1$, and
\[
  c_\infty = \textrm{ev}_\infty(P)\textrm{ev}_\infty(Q) = a_1b_1
\]
is obtained via evaluation at the point at infinity, where the evaluation at
infinity $\textrm{ev}_{\infty}$ is the function mapping a polynomial to its
leading coefficient. This strategy can be generalized to curves (or their
function fields) more complex
than $\mathbb{P}^1(\K)$, as was done by Chudnovsky and Chudnovsky in
$1988$~\cite{CC88}.

\subsection{Asymptotic complexity}

In 1988, Chudnovsky and Chudnovsky~\cite{CC88} extended the idea of polynomial
interpolation to interpolation on rational places, \ie places of degree $1$, of
a function field. It led to an algorithm for the finite field product with an
asymptotically linear complexity in the extension degree. We first present the historical theorem
in~\cite{CC88}.
\begin{thm}
  \label{thm:CC88}
  Let $F$ be a function field over $\mathbb{F}_q$.
  Assume there exist a place $Q\in\mathbb{P}_{F}$ of $F$ of degree $k$, $P_1,
  \dots, P_n\in\mathbb{P}_F$ places of $F$ of degree $1$, and a divisor
  $D\in\D_F$ of $F$ such that the places $Q$ and $P_1, \dots, P_n$ are not in
  the support of $D$ and such that the following conditions hold.
  \begin{enumerate}[(i)]
    \item \label{cond:1} The evaluation map
      \[
        \begin{array}{cccc}
        \ev_{Q, D}: & L(D) & \to & \mathbb{F}_{q^k}\\
  & f & \mapsto & f(Q)
\end{array}
\]
is \emph{surjective}.
    \item \label{cond:2} The evaluation map
      \[
        \begin{array}{cccc}
        \ev_{\Pcal, 2D}: & L(2D) & \to & (\mathbb{F}_{q})^n\\
  & h & \mapsto & (h(P_1), \dots, h(P_n))
\end{array}
\]
is \emph{injective}.
  \end{enumerate}
  Then the product in the extension field 
  \[
    \mathbb{F}_{q^k}/\mathbb{F}_q
  \]
  admits a symmetric formula of length $n$, \ie we have $\musym_q(k)\leq n$.
\end{thm}

\section{Algorithmic searches in small dimension}

% Contents
% ========
%
% - Barbulescu, Detrey, et al 
% - Covanov 

The last results based on Chudnovsky and Chudnovsky algorithm allow us to find
one decomposition, and thus give us an (asymptotic) upper bound on the bilinear
complexity of algebras
\[
  \A = \mathbb{F}_{q^k}[T]/(T^l).
\]
When one wants to find the exact value of the bilinear complexity of a given
bilinear map $\Phi$, one can either find all bilinear formulas for $\Phi$, or
find a theoretical argument to directly find the bilinear complexity of $\Phi$. The latter
solution is often hard, and there also exist some in-between approaches such
as finding a bilinear formula of a given length and proving that no shorter formula
could exist. Still, because it seems hard to directly find the bilinear
complexity of a bilinear map, algorithms were developped to find bilinear
formulas. These algorithms are essentially exhaustive searches, so they 
have an exponential complexity, but they also exploit the eventual symmetries
in the definition of $\Phi$ to eliminate a lot of potential candidates along the
way, in order to be as efficient as possible in practice. We first see
Barbulescu \emph{et al} algorithm~\cite{BDEZ12}.

\subsection{Barbulescu, Detrey, Estibals and Zimmerman's algorithm}

In $2012$, Barbulescu, Detrey, Estibals and Zimmerman published a new framework
to find bilinear formulas for arbitrary bilinear maps over finite fields. Let
$V_1$, $V_2$ and $W$ be three finite dimensional $\K$-vector space of respective
dimension $l$, $m$, and $n$, such that we have
\[
  V_1\cong\K^l\hspace{3cm}V_2\cong\K^m\hspace{3cm}W\cong\K^n.
\]
Let
\[
  \Phi:V_1\times V_2\to W
\]
be a bilinear map, and denote by $\B$ the space of bilinear \emph{forms} from
$V_1\times V_2$ to $\K$. Let $\gamma\in\B$ a bilinear form, then if
\[
  x = (x_1, \dots, x_l)\in V_1
\]
and
\[
  y = (y_1, \dots, y_m)\in V_2,
\]
then $\gamma$ is given by
\[
  \gamma(x, y) = \sum_{i=1}^{l}\sum_{j=1}^m \gamma_{i, j} x_i y_j.
\]
Hence, we can see $\B$ as a $\K$-vector space of dimension $lm$ and $\gamma$ as
a vector
\[
  \gamma = (\gamma_{1, 1}, \dots, \gamma_{l, m}).
\]
An other interesting representation is to see $\gamma$ as a $l\times m$ matrix
\[
  G = (\gamma_{i, j})_{i, j}
\]
such that $\gamma$ is given by
\[
  \gamma(x, y) = x G y^t
\]
where $y^t$ is the transposed of $y$ and thus $y^t$ is a column vector. In this
setting, the \emph{bilinear complexity} of $\gamma$ is directly given by the
rank of $G$. Indeed, a rank $r$ matrix is the sum of $r$ matrices of rank $1$,
and a rank $1$ matrix corresponds to a rank $1$ bilinear form.
% 
